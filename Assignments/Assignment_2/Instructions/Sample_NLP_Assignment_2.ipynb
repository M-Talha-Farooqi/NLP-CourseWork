{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c85a464e-6a7b-4035-8c52-54cf67c6ce9c",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b> **NLP and Machine Learning Assignment: Sentiment Analysis on Reviews Dataset**</b></div>\n",
    "\n",
    "</div>\n",
    "\n",
    "## **Objective**\n",
    "The goal of this assignment is to learn how to process textual data, extract features, train various machine learning models, and evaluate their performance on a reviews dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 1: Setup and Data Loading**\n",
    "\n",
    "1. **Load the Dataset**  \n",
    "   - Use `Pandas` to read the dataset from a CSV file.\n",
    "   - Display the first few rows of the dataset to understand its structure.\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 2: Data Preprocessing**\n",
    "\n",
    "Preprocess the text data using the following steps:\n",
    "\n",
    "1. **Convert all text to lowercase**  \n",
    "2. **Remove all non-alphanumeric characters (punctuation, numbers, etc.)**  \n",
    "3. **Tokenize the text**  \n",
    "4. **Remove stop words using NLTK**  \n",
    "5. **Apply lemmatization using WordNetLemmatizer from NLTK**\n",
    "6. **Remove the URL's**\n",
    "7. **Any Possible Processing**\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 3: Feature Extraction**\n",
    "\n",
    "Extract features using two different methods:\n",
    "\n",
    "1. **Bag of Words (Frequency Count)**  \n",
    "   - Use `CountVectorizer` from `sklearn` to extract features.\n",
    "\n",
    "2. **TF-IDF**  \n",
    "   - Use `TfidfVectorizer` from `sklearn` to extract features.\n",
    "  \n",
    "3. **Combine Bag of Words (Frequency Count) and TF-IDF Features**\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 4: Data Splitting**\n",
    "\n",
    "Split the data into training and test sets:\n",
    "\n",
    "1. Use `train_test_split` from `sklearn` to split the data.\n",
    "2. Use 80% of the data for training and 20% for testing.\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 5: Model Training**\n",
    "\n",
    "Train three different machine learning models:\n",
    "\n",
    "1. **Random Forest**  \n",
    "2. **Support Vector Machine (SVM)**  \n",
    "3. **Naive Bayes**  \n",
    "\n",
    "- Use `sklearn`'s implementations for these models.\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 6: Evaluation**\n",
    "\n",
    "Evaluate each model on the test data:\n",
    "\n",
    "1. Calculate and print the following metrics:\n",
    "   - **Accuracy**\n",
    "   - **Precision**\n",
    "   - **Recall**\n",
    "   - **F1-score**\n",
    "   - **Confusion Matrix**\n",
    "   - **Classification Report**\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 7: Comparative Analysis**\n",
    "\n",
    "Create a comparison graph of the model performance metrics:\n",
    "\n",
    "1. Plot a bar graph comparing the **Accuracy**, **Precision**, **Recall**, and **F1-score** for each model.\n",
    "2. Use `matplotlib` or `seaborn` for plotting.\n",
    "\n",
    "---\n",
    "\n",
    "## **Part 8: Submission**\n",
    "\n",
    "1. **Submit a Jupyter Notebook**  \n",
    "   - Ensure the notebook contains the completed code for all parts.er Notebook**  \n",
    "   - Ensure the notebook contains the completed code for all parts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc213462-faf1-47b3-8956-2389c288fd2d",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b> Step by Step Implementation</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31b764-e5eb-466f-8e5d-9bc2806deb8f",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:24px;font-family:Georgia;text-align:Left;display:fill;border-radius:10px;background-color:#254E58;overflow:hidden\"><b> Import Required Libraries</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7e27f3e-fa9c-4f06-9cf8-7096486fc696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\waqar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\waqar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Handling and Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Model Training\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Data Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download necessary NLTK data files (only need to run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c229a-92be-42ce-800c-7dd7d69bcf0a",
   "metadata": {},
   "source": [
    "## **Part 1: Setup and Data Loading**\n",
    "\n",
    "1. **Load the Dataset**  \n",
    "   - Use `Pandas` to read the dataset from a CSV file.\n",
    "   - Display the first few rows of the dataset to understand its structure.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f22d2b12-76db-4170-8e06-53a2492b6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code TODO\n",
    "# Must Display the output in Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c6449d-473b-43fe-a4d0-c35819db194e",
   "metadata": {},
   "source": [
    "## **Part 2: Data Preprocessing**\n",
    "\n",
    "Preprocess the text data using the following steps:\n",
    "\n",
    "1. **Convert all text to lowercase**  \n",
    "2. **Remove all non-alphanumeric characters (punctuation, numbers, etc.)**  \n",
    "3. **Tokenize the text**  \n",
    "4. **Remove stop words using NLTK**  \n",
    "5. **Apply lemmatization using WordNetLemmatizer from NLTK**\n",
    "6. **Remove the URL's**\n",
    "7. **Any Possible Processing**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19aa60cb-f783-48b2-985c-74f3479827ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code TODO\n",
    "# Must Display the output in Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd870ed-55c8-4ad2-bb3b-b23578ffe4cf",
   "metadata": {},
   "source": [
    "## **Part 3: Feature Extraction**\n",
    "\n",
    "Extract features using two different methods:\n",
    "\n",
    "1. **Bag of Words (Frequency Count)**  \n",
    "   - Use `CountVectorizer` from `sklearn` to extract features.\n",
    "\n",
    "2. **TF-IDF**  \n",
    "   - Use `TfidfVectorizer` from `sklearn` to extract features.\n",
    "     \n",
    "3. **Combine Bag of Words (Frequency Count) and TF-IDF Features**\n",
    "\n",
    "\n",
    "## Must SetUp the following Vectorizer Parameters\n",
    "\n",
    "In both `CountVectorizer` and `TfidfVectorizer`, we can customize the way text data is transformed into features using various parameters. In this assignment must setup the below  given parameters with explanations:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Example with CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    token_pattern=r'(?u)\\\\b\\\\w\\\\w+\\\\b',  # Matches words with two or more alphanumeric characters\n",
    "    ngram_range=(1, 1),                  # Only includes unigrams (single words)\n",
    "    analyzer='word',                     # Analyzes text by splitting into words (not characters)\n",
    "    max_features=10                    # Considers only 10 unique terms \n",
    ")\n",
    "\n",
    "# Example with TfidfVectorizer (same parameters apply)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    token_pattern=r'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_features=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1c1ce4-0d0d-4048-87a8-e3748602dd60",
   "metadata": {},
   "source": [
    "### Note: Must print the features in a Dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb20abf0-21a4-47f2-a0e9-f49e90d02717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef67c12-ef34-4a3e-b1e1-e107765c0d89",
   "metadata": {},
   "source": [
    "## **Part 4: Data Splitting**\n",
    "\n",
    "Split the data into training and test sets:\n",
    "\n",
    "1. Use `train_test_split` from `sklearn` to split the data.\n",
    "2. Use 80% of the data for training and 20% for testing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52a98285-9173-4d03-9160-9c8d8908c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code TODO\n",
    "# Must Display the output in Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da84b3e-f3b1-4d4e-a753-d0faadffdc4a",
   "metadata": {},
   "source": [
    "## **Part 5: Model Training**\n",
    "\n",
    "Train three different machine learning models:\n",
    "\n",
    "1. **Random Forest**  \n",
    "2. **Support Vector Machine (SVM)**  \n",
    "3. **Naive Bayes**  \n",
    "\n",
    "- Use `sklearn`'s implementations for these models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85c78ec3-6f79-44d0-8cbd-115efe3f3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d3509-ef17-45f3-8c03-c5c90fd0e20c",
   "metadata": {},
   "source": [
    "## **Part 6: Evaluation**\n",
    "\n",
    "Evaluate each model on the test data:\n",
    "\n",
    "1. Calculate and print the following metrics:\n",
    "   - **Accuracy**\n",
    "   - **Precision**\n",
    "   - **Recall**\n",
    "   - **F1-score**\n",
    "   - **Confusion Matrix**\n",
    "   - **Classification Report**\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a5f9d52-a3c3-428d-99ac-369f31a7ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code TODO\n",
    "# Must Display the output in Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5787951b-617a-4c66-ad9e-5c0cdd0d7273",
   "metadata": {},
   "source": [
    "## **Part 7: Comparative Analysis**\n",
    "\n",
    "Create a comparison graph of the model performance metrics:\n",
    "\n",
    "1. Plot a bar graph comparing the **Accuracy**, **Precision**, **Recall**, and **F1-score** for each model.\n",
    "2. Use `matplotlib` or `seaborn` for plotting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57a35da7-d11b-4e3d-ab79-be8178ab5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fba6b6-0da8-42c6-bc4a-8b18e5643b7a",
   "metadata": {},
   "source": [
    "## **Part 8: Submission**\n",
    "\n",
    "1. **Submit a Jupyter Notebook**  \n",
    "   - Ensure the notebook contains the completed code for all parts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
